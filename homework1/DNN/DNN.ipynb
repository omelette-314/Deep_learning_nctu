{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784, 1) (12000, 10, 1)\n",
      "(5768, 784, 1) (5768, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "data=np.load(r'train.npz')\n",
    "image, label= data['image'], data['label']\n",
    "#image_data, label_data = data['image'][..., None], data['label'].astype('int64')\n",
    "train_x = np.zeros((len(image),28*28),dtype='int64')\n",
    "train_y = np.zeros((len(label),10),dtype='int64')\n",
    "\n",
    "for i in range(len(image)):\n",
    "    train_y[i,int(label[i])]=1\n",
    "    for j in range(28):\n",
    "        for k in range(28):\n",
    "            train_x[i,k+28*j]=int(image[i,j,k])\n",
    "train_x = train_x.reshape(len(image), -1, 1)\n",
    "train_y = train_y.reshape(len(label), -1, 1)\n",
    "print(train_x.shape,train_y.shape)\n",
    "training_data = list(zip(train_x, train_y))\n",
    "data=np.load(r'test.npz')\n",
    "image, label= data['image'], data['label']\n",
    "test_x = np.zeros((len(image),28*28),dtype='int64')\n",
    "test_y = np.zeros((len(label),10),dtype='int64')\n",
    "\n",
    "for i in range(len(image)):\n",
    "    test_y[i,int(label[i])]=1\n",
    "    for j in range(28):\n",
    "        for k in range(28):\n",
    "            test_x[i,k+28*j]=int(image[i,j,k])\n",
    "test_x = test_x.reshape(len(image), -1, 1)\n",
    "test_y = test_y.reshape(len(label), -1, 1)\n",
    "print(test_x.shape,test_y.shape)\n",
    "testing_data = list(zip(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 128\n",
      "32 64\n",
      "2 32\n",
      "10 2\n",
      "128 64\n",
      "64 32\n",
      "32 2\n",
      "2 10\n",
      "64 128\n",
      "32 64\n",
      "2 32\n",
      "10 2\n",
      "[[1.73392602 1.13440008]]\n",
      "<class 'numpy.ndarray'>\n",
      "2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "neurons=[128,64,32,2,10]\n",
    "for i,j in zip(neurons[1:],neurons[:-1]):\n",
    "    print(i,j)\n",
    "for i, j in zip(neurons[:-1], neurons[1:]):\n",
    "    print(i,j)\n",
    "for x, y in zip(neurons[:-1], neurons[1:]):\n",
    "    print(y,x)\n",
    "#[np.random.randn(y, x) for x, y in zip(neurons[:-1], neurons[1:])]\n",
    "print(np.random.randn(1,2))\n",
    "print(type(np.array(neurons)))\n",
    "print(np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.4]\n"
     ]
    }
   ],
   "source": [
    "print(1-np.array([0.7,0.6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = np.log(np.maximum(z, 1e-9))\n",
    "    exps = np.exp(z)    # -max(z).all()\n",
    "    sums = np.sum(exps) # -max(z).all()\n",
    "    return np.divide(exps, sums)\n",
    "def sigmoid(z):\n",
    "    for (x, y), val in np.ndenumerate(z):\n",
    "        if val >= 100:\n",
    "            z[x][y] = 1.\n",
    "        elif val <= -100:\n",
    "            z[x][y] = 0.\n",
    "        else:\n",
    "            z[x][y] = 1.0 / (1.0 + np.exp(-val))\n",
    "    return z\n",
    "def softmax_derivate(z):\n",
    "    z=z.flatten()\n",
    "    jacobian_m = np.diag(z)\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                #print(i,j)\n",
    "                #print(z.shape)\n",
    "                jacobian_m[i][j] = z[i] * (1-z[i])\n",
    "            else: \n",
    "                jacobian_m[i][j] = -z[i]*z[j]\n",
    "    return jacobian_m\n",
    "#def sigmoid(z):\n",
    "#    return 1.0 / (1.0 + np.exp(-z)) 不設限制很容易爆炸\n",
    "def sigmoid_derivate(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "def relu_derivative(z):\n",
    "    return np.where(z <= 0, 0, 1)\n",
    "def cross_entropy(output,ground_truth):\n",
    "    output = np.clip(output, 1e-9, 1. - 1e-9)\n",
    "    ce = -(np.sum(np.nan_to_num(ground_truth*np.log(output)))) /len(output)\n",
    "    return ce\n",
    "#def bin_cross_entropy(output, ground_truth):\n",
    "#    return np.sum( np.nan_to_num( -ground_truth*(np.log(output)) - (1-ground_truth)*(np.log(1-output)) ) )\n",
    "def cross_entropy_derivative(output, ground_truth): #softmax and cross_entropy derivate\n",
    "    return output - ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN():\n",
    "    def __init__(self,neurons,rd=False):\n",
    "        self.num_layers=len(neurons)\n",
    "        self.neurons=neurons\n",
    "        if rd==True:\n",
    "            self.random_initial()\n",
    "        else:\n",
    "            self.zero_initial()\n",
    "        self.training_loss = []\n",
    "        self.training_error_rate = []\n",
    "        self.testing_error_rate = []\n",
    "    def random_initial(self):\n",
    "        self.weights =[np.random.randn(i,j)for i,j in zip(neurons[1:],neurons[:-1])] #random_normal number =len(neurons-1)\n",
    "        self.biases  =[np.random.randn(j,1)for j in neurons[1:]] #add after data mutiby weight so (m,1)\n",
    "        print(\"random_initial\")\n",
    "    def zero_initial(self):\n",
    "        self.weights =[np.zeros((i,j),dtype='float')for i,j in zip(neurons[1:],neurons[:-1])] # transposed (n,m)->(m,n)\n",
    "        self.biases  =[np.zeros((j,1),dtype='float')for j in neurons[1:]] #add after data mutiby weight so (m,1)\n",
    "        print(\"zero_initial\")\n",
    "    def SGD(self,train_data,test_data,epochs,batch_size,lr): #randomly choose data as batch size\n",
    "        num = len(train_data)\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(train_data)\n",
    "            mini_batch = [train_data[j:j+batch_size] for j in range(0,num,batch_size) ]\n",
    "            for data in mini_batch:\n",
    "                self.update_mini_batch(data,lr)\n",
    "            if (i==20):\n",
    "                w_20 = self.weights\n",
    "                b_20 = self.biases\n",
    "            if (i==80):\n",
    "                w_80 = self.weights\n",
    "                b_80 = self.biases\n",
    "            if (i % 50 == 0):\n",
    "                # record info\n",
    "                self.training_loss.append(self.calc_loss(training_data))\n",
    "                self.training_error_rate.append(self.count_error(training_data) / len(training_data))\n",
    "                self.testing_error_rate.append(self.count_error(testing_data) / len(testing_data))\n",
    "                print('===================================')\n",
    "                print(\"【Epoch %s】\" % i) \n",
    "                print('    training loss: %f' % self.calc_loss(training_data))\n",
    "                print('    training error rate: %d / %d(%f)' % (self.count_error(training_data), len(training_data), self.count_error(training_data) / len(training_data)))\n",
    "                print('    testing error rate: %d / %d(%f)' % (self.count_error(testing_data), len(testing_data), self.count_error(testing_data) / len(testing_data)))\n",
    "\n",
    "    def update_mini_batch(self, batch_data, lr):\n",
    "        sum_gradient_w = [ np.zeros(w.shape) for w in self.weights ] #creat same size as weight think as 暫存器\n",
    "        sum_gradient_b = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        # cumulate gradient of each single data\n",
    "        for x, y in batch_data:\n",
    "            gradient_w, gradient_b = self.backward(x, y)\n",
    "            sum_gradient_w = [ sw + w for sw, w in zip(sum_gradient_w, gradient_w)] #sum up to update \n",
    "            sum_gradient_b = [ sb + b for sb, b in zip(sum_gradient_b, gradient_b)] #gradient afer each batch\n",
    "        # update weights & biases with (mean of sum of gradient * learning rate)\n",
    "        #print(sum_gradient_w,sum_gradient_b)\n",
    "        self.weights = [ w - lr/len(batch_data) * sw for w, sw in zip(self.weights, sum_gradient_w) ]# weight-average_weight*lr\n",
    "        self.biases = [ b - lr/len(batch_data) * sb for b, sb in zip(self.biases, sum_gradient_b) ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        z=[] # use to derivation\n",
    "        activation_z=[x]\n",
    "        for layer in range(self.num_layers-1):\n",
    "            if layer ==(self.num_layers-2):\n",
    "                #print(self.weights[4].shape,self.biases[4].shape)\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=softmax(x)\n",
    "                activation_z.append(x)\n",
    "            else:\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=sigmoid(x)\n",
    "                activation_z.append(x)\n",
    "        return x,z,activation_z\n",
    "    def backward(self,x,y):\n",
    "        #create tmp weight and bias\n",
    "        gradient_w = [ np.zeros(w.shape) for w in self.weights ] #0~len(neouron)-2\n",
    "        gradient_b = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        # forward\n",
    "        _,zs,activation_z=self.forward(x)\n",
    "        # backward \n",
    "        # we calc last layer separately, from back to forward\n",
    "        for layer in range(1,self.num_layers):\n",
    "            if layer == 1: #first initial\n",
    "                delta = cross_entropy_derivative(activation_z[-layer],y)\n",
    "                #delta = cross_entropy_derivative(activation_z[-layer],y) * sigmoid_derivate(zs[-1])\n",
    "                #delta = softmax_derivate(z[-1])\n",
    "                gradient_b[-layer]=delta * 1\n",
    "                gradient_w[-layer]=np.dot(delta,activation_z[-layer-1].T)\n",
    "            else:\n",
    "                delta = np.dot(self.weights[-layer + 1].T, delta) * sigmoid_derivate(zs[-layer])\n",
    "                #delta = np.dot(self.weights[-layer + 1].T, delta) * relu_derivate(zs[-layer])\n",
    "                gradient_w[-layer] = np.dot(delta, activation_z[-layer - 1].T)\n",
    "                gradient_b[-layer] = delta\n",
    "        return gradient_w, gradient_b\n",
    "    def calc_loss(self,data):\n",
    "    # calc cross entropy loss\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            output,_,_ = self.forward(x)\n",
    "            #output = self.forward(x)\n",
    "            #print(\"Output:{},Ground Truth:{}\".format(output,y))\n",
    "            loss += cross_entropy(output, y) #/ len(data)\n",
    "        return loss\n",
    "    def count_error(self, data):\n",
    "    # count error number\n",
    "        compare_list=[]\n",
    "        for x , y in data:\n",
    "            #x = self.forward(x)\n",
    "            x,_,_ = self.forward(x)\n",
    "            x = x.argmax()\n",
    "            y = np.argmax(y)\n",
    "            #print(x,y)\n",
    "            compare_list.append([x,y])\n",
    "        #compare_list = [ (np.argmax(self.forward(x)), np.argmax(y)) for x, y in data ]\n",
    "        error_count = sum( int(y1 != y2) for y1, y2 in compare_list)\n",
    "        return error_count \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        z=[] # use to derivation\n",
    "        activation_z=[x]\n",
    "        for layer in range(self.num_layers-1):\n",
    "            if layer ==(self.num_layers-2):\n",
    "                #print(self.weights[4].shape,self.biases[4].shape)\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=softmax(x)\n",
    "                activation_z.append(x)\n",
    "            else:\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=sigmoid(x)\n",
    "                activation_z.append(x)\n",
    "        return x,z,activation_z\n",
    "#    def forward(self,x):\n",
    "#        for w,b in zip(self.weights,self.biases):\n",
    "#            x = np.dot(w,x)+b\n",
    "#            x = sigmoid(x)\n",
    "#        return x\n",
    "        activation = x\n",
    "        activation_z = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activation_z.append(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-9320ba7591b7>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-9320ba7591b7>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    bias_20 = =self.biases\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class NN():\n",
    "    def __init__(self, neurons):\n",
    "        self.num_layers = len(neurons)\n",
    "        self.neurons = neurons\n",
    "        # create weights & bias\n",
    "        self.weights = [ np.zeros((j, i)) for i, j in zip(neurons[:-1], neurons[1:]) ]\n",
    "        self.biases = [ np.zeros((i, 1)) for i in neurons[1:] ]\n",
    "        # info \n",
    "        self.training_loss = []\n",
    "        self.training_error_rate = []\n",
    "        self.testing_error_rate = []\n",
    "    \n",
    "    def SGD(self, training_data, testing_data, epochs, batch_size, lr):\n",
    "\n",
    "        num = len(training_data)\n",
    "        self.training_loss = []\n",
    "        self.training_error_rate = []\n",
    "        self.testing_error_rate = []\n",
    "        \n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batch = [ training_data[i : i + batch_size] for i in range(0, num, batch_size) ] # split data into mini_batch\n",
    "            for single_data in mini_batch:\n",
    "                self.update_mini_batch(single_data, lr)\n",
    "            if (epoch == 20):\n",
    "                weight_20 = self.weights\n",
    "                bias_20 = =self.biases\n",
    "            if (epoch == 80):\n",
    "                weight_80 = self.weights\n",
    "                bias_80 = self.biases\n",
    "            if (epoch % 50 == 0):\n",
    "                # record info\n",
    "                self.training_loss.append(self.calc_loss(training_data))\n",
    "                self.training_error_rate.append(self.count_error(training_data) / len(training_data))\n",
    "                self.testing_error_rate.append(self.count_error(testing_data) / len(testing_data))\n",
    "                print('===================================')\n",
    "                print(\"【Epoch %s】\" % epoch) \n",
    "                print('    training loss: %f' % self.calc_loss(training_data))\n",
    "                print('    training error rate: %d / %d(%f)' % (self.count_error(training_data), len(training_data), self.count_error(training_data) / len(training_data)))\n",
    "                print('    testing error rate: %d / %d(%f)' % (self.count_error(testing_data), len(testing_data), self.count_error(testing_data) / len(testing_data)))\n",
    "        return weight_20,bias_20,weight_80,bias_80\n",
    "                \n",
    "    \n",
    "    def update_mini_batch(self, single_data, lr):\n",
    "        sum_gradient_w = [ np.zeros(w.shape) for w in self.weights ]\n",
    "        sum_gradient_b = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        \n",
    "        # cumulate gradient of each single data\n",
    "        for x, y in single_data:\n",
    "            gradient_w, gradient_b = self.backward(x, y)\n",
    "            sum_gradient_w = [  sw + w for sw, w in zip(sum_gradient_w, gradient_w)]\n",
    "            sum_gradient_b = [ sb + b for sb, b in zip(sum_gradient_b, gradient_b)]\n",
    "        \n",
    "        # update weights & biases with (mean of sum of gradient * learning rate)\n",
    "        self.weights = [ w - lr/len(single_data) * sw for w, sw in zip(self.weights, sum_gradient_w) ]\n",
    "        self.biases = [ b - lr/len(single_data) * sb for b, sb in zip(self.biases, sum_gradient_b) ]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            x = np.dot(w, x) + b\n",
    "            x = sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        # store gradient of w, b\n",
    "        gradient_w = [ np.zeros(w.shape) for w in self.weights ]\n",
    "        gradient_b = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        \n",
    "        # forward\n",
    "        activation = x\n",
    "        zs = [] # store vectors which is input of activation function\n",
    "        activations = [x] # store vectors which is output of activation function\n",
    "        \n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward \n",
    "        # we calc last layer separately, because loss function is diff with activation funcion\n",
    "        delta = cross_entropy_derivative(activations[-1], y)\n",
    "        gradient_b[-1] = delta * 1\n",
    "        gradient_w[-1] = np.dot(delta, activations[-2].T)\n",
    "        for layer in range(2, self.num_layers):\n",
    "            z = zs[-layer]\n",
    "            delta = np.dot(self.weights[-layer + 1].T, delta) * sigmoid_derivate(z)\n",
    "            gradient_w[-layer] = np.dot(delta, activations[-layer - 1].T)\n",
    "            gradient_b[-layer] = delta\n",
    "        return gradient_w, gradient_b\n",
    "        \n",
    "    def calc_loss(self, data):\n",
    "        # calc cross entropy loss\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            output = self.forward(x)\n",
    "            loss += cross_entropy(output, y)/ len(data)\n",
    "        return loss\n",
    "    \n",
    "    def count_error(self, data):\n",
    "        # count error number\n",
    "        compare_list = [ (np.argmax(self.forward(x)), np.argmax(y)) for x, y in data ]\n",
    "        error_count = sum( int(y1 != y2) for y1, y2 in compare_list)\n",
    "        return error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[784,128,2,10]\n",
    "#neurons=[784,100,10]\n",
    "\n",
    "#module1 = DNN(neurons,True)\n",
    "module1 = NN(neurons)\n",
    "w_2,b_2,w_8,b_8=module1.SGD(training_data, testing_data, 100, 60, 0.08)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
