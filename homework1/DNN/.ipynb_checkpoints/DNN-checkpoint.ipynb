{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784, 1) (12000, 10, 1)\n",
      "(5768, 784, 1) (5768, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "data=np.load(r'train.npz')\n",
    "image, label= data['image'], data['label']\n",
    "#image_data, label_data = data['image'][..., None], data['label'].astype('int64')\n",
    "train_x = np.zeros((len(image),28*28),dtype='int64')\n",
    "train_y = np.zeros((len(label),10),dtype='int64')\n",
    "\n",
    "for i in range(len(image)):\n",
    "    train_y[i,int(label[i])]=1\n",
    "    for j in range(28):\n",
    "        for k in range(28):\n",
    "            train_x[i,k+28*j]=int(image[i,j,k])\n",
    "train_x = train_x.reshape(len(image), -1, 1)\n",
    "train_y = train_y.reshape(len(label), -1, 1)\n",
    "print(train_x.shape,train_y.shape)\n",
    "training_data = list(zip(train_x, train_y))\n",
    "data=np.load(r'test.npz')\n",
    "image, label= data['image'], data['label']\n",
    "test_x = np.zeros((len(image),28*28),dtype='int64')\n",
    "test_y = np.zeros((len(label),10),dtype='int64')\n",
    "\n",
    "for i in range(len(image)):\n",
    "    test_y[i,int(label[i])]=1\n",
    "    for j in range(28):\n",
    "        for k in range(28):\n",
    "            test_x[i,k+28*j]=int(image[i,j,k])\n",
    "test_x = test_x.reshape(len(image), -1, 1)\n",
    "test_y = test_y.reshape(len(label), -1, 1)\n",
    "print(test_x.shape,test_y.shape)\n",
    "testing_data = list(zip(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 128\n",
      "32 64\n",
      "2 32\n",
      "10 2\n",
      "128 64\n",
      "64 32\n",
      "32 2\n",
      "2 10\n",
      "64 128\n",
      "32 64\n",
      "2 32\n",
      "10 2\n",
      "[[-1.59882138 -1.21682571]]\n",
      "<class 'numpy.ndarray'>\n",
      "2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "neurons=[128,64,32,2,10]\n",
    "for i,j in zip(neurons[1:],neurons[:-1]):\n",
    "    print(i,j)\n",
    "for i, j in zip(neurons[:-1], neurons[1:]):\n",
    "    print(i,j)\n",
    "for x, y in zip(neurons[:-1], neurons[1:]):\n",
    "    print(y,x)\n",
    "#[np.random.randn(y, x) for x, y in zip(neurons[:-1], neurons[1:])]\n",
    "print(np.random.randn(1,2))\n",
    "print(type(np.array(neurons)))\n",
    "print(np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.4]\n"
     ]
    }
   ],
   "source": [
    "print(1-np.array([0.7,0.6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = np.log(np.maximum(z, 1e-9))\n",
    "    exps = np.exp(z)    # -max(z).all()\n",
    "    sums = np.sum(exps) # -max(z).all()\n",
    "    return np.divide(exps, sums)\n",
    "def sigmoid(z):\n",
    "    for (x, y), val in np.ndenumerate(z):\n",
    "        if val >= 100:\n",
    "            z[x][y] = 1.\n",
    "        elif val <= -100:\n",
    "            z[x][y] = 0.\n",
    "        else:\n",
    "            z[x][y] = 1.0 / (1.0 + np.exp(-val))\n",
    "    return z\n",
    "def softmax_derivate(z):\n",
    "    z=z.flatten()\n",
    "    jacobian_m = np.diag(z)\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                #print(i,j)\n",
    "                #print(z.shape)\n",
    "                jacobian_m[i][j] = z[i] * (1-z[i])\n",
    "            else: \n",
    "                jacobian_m[i][j] = -z[i]*z[j]\n",
    "    return jacobian_m\n",
    "#def sigmoid(z):\n",
    "#    return 1.0 / (1.0 + np.exp(-z)) 不設限制很容易爆炸\n",
    "def sigmoid_derivate(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "def relu_derivative(z):\n",
    "    return np.where(z <= 0, 0, 1)\n",
    "def cross_entropy(output,ground_truth):\n",
    "    output = np.clip(output, 1e-9, 1. - 1e-9)\n",
    "    ce = -(np.sum(np.nan_to_num(ground_truth*np.log(output)))) /len(output)\n",
    "    return ce\n",
    "#def bin_cross_entropy(output, ground_truth):\n",
    "#    return np.sum( np.nan_to_num( -ground_truth*(np.log(output)) - (1-ground_truth)*(np.log(1-output)) ) )\n",
    "def cross_entropy_derivative(output, ground_truth): #softmax and cross_entropy derivate\n",
    "    return output - ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN():\n",
    "    def __init__(self,neurons,rd=False):\n",
    "        self.num_layers=len(neurons)\n",
    "        self.neurons=neurons\n",
    "        if rd==True:\n",
    "            self.random_initial()\n",
    "        else:\n",
    "            self.zero_initial()\n",
    "        self.training_loss = []\n",
    "        self.training_error_rate = []\n",
    "        self.testing_error_rate = []\n",
    "    def random_initial(self):\n",
    "        self.weights =[np.random.randn(i,j)for i,j in zip(neurons[1:],neurons[:-1])] #random_normal number =len(neurons-1)\n",
    "        self.biases  =[np.random.randn(j,1)for j in neurons[1:]] #add after data mutiby weight so (m,1)\n",
    "        print(\"random_initial\")\n",
    "    def zero_initial(self):\n",
    "        self.weights =[np.zeros((i,j),dtype='float')for i,j in zip(neurons[1:],neurons[:-1])] # transposed (n,m)->(m,n)\n",
    "        self.biases  =[np.zeros((j,1),dtype='float')for j in neurons[1:]] #add after data mutiby weight so (m,1)\n",
    "        print(\"zero_initial\")\n",
    "    def SGD(self,train_data,test_data,epochs,batch_size,lr): #randomly choose data as batch size\n",
    "        num = len(train_data)\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(train_data)\n",
    "            mini_batch = [train_data[j:j+batch_size] for j in range(0,num,batch_size) ]\n",
    "            for data in mini_batch:\n",
    "                self.update_mini_batch(data,lr)    \n",
    "            if (i % 50 == 0):\n",
    "                # record info\n",
    "                self.training_loss.append(self.calc_loss(training_data))\n",
    "                self.training_error_rate.append(self.count_error(training_data) / len(training_data))\n",
    "                self.testing_error_rate.append(self.count_error(testing_data) / len(testing_data))\n",
    "                print('===================================')\n",
    "                print(\"【Epoch %s】\" % i) \n",
    "                print('    training loss: %f' % self.calc_loss(training_data))\n",
    "                print('    training error rate: %d / %d(%f)' % (self.count_error(training_data), len(training_data), self.count_error(training_data) / len(training_data)))\n",
    "                print('    testing error rate: %d / %d(%f)' % (self.count_error(testing_data), len(testing_data), self.count_error(testing_data) / len(testing_data)))\n",
    "\n",
    "    def update_mini_batch(self, batch_data, lr):\n",
    "        sum_gradient_w = [ np.zeros(w.shape) for w in self.weights ] #creat same size as weight think as 暫存器\n",
    "        sum_gradient_b = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        # cumulate gradient of each single data\n",
    "        for x, y in batch_data:\n",
    "            gradient_w, gradient_b = self.backward(x, y)\n",
    "            sum_gradient_w = [ sw + w for sw, w in zip(sum_gradient_w, gradient_w)] #sum up to update \n",
    "            sum_gradient_b = [ sb + b for sb, b in zip(sum_gradient_b, gradient_b)] #gradient afer each batch\n",
    "        # update weights & biases with (mean of sum of gradient * learning rate)\n",
    "        #print(sum_gradient_w,sum_gradient_b)\n",
    "        self.weights = [ w - lr/len(batch_data) * sw for w, sw in zip(self.weights, sum_gradient_w) ]# weight-average_weight*lr\n",
    "        self.biases = [ b - lr/len(batch_data) * sb for b, sb in zip(self.biases, sum_gradient_b) ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        z=[] # use to derivation\n",
    "        activation_z=[x]\n",
    "        for layer in range(self.num_layers-1):\n",
    "            if layer ==(self.num_layers-2):\n",
    "                #print(self.weights[4].shape,self.biases[4].shape)\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=softmax(x)\n",
    "                activation_z.append(x)\n",
    "            else:\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=sigmoid(x)\n",
    "                activation_z.append(x)\n",
    "        return x,z,activation_z\n",
    "    def backward(self,x,y):\n",
    "        #create tmp weight and bias\n",
    "        gradient_w = [ np.zeros(w.shape) for w in self.weights ] #0~len(neouron)-2\n",
    "        gradient_b = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        # forward\n",
    "        _,zs,activation_z=self.forward(x)\n",
    "        # backward \n",
    "        # we calc last layer separately, from back to forward\n",
    "        for layer in range(1,self.num_layers):\n",
    "            if layer == 1: #first initial\n",
    "                #delta = cross_entropy_derivative(activation_z[-1],y)\n",
    "                delta = cross_entropy_derivative(activation_z[-layer],y) #* sigmoid_derivate(zs[-1])\n",
    "                #delta = softmax_derivate(z[-1])\n",
    "                gradient_b[-layer]=delta * 1\n",
    "                gradient_w[-layer]=np.dot(delta,activation_z[-layer-1].T)\n",
    "            else:\n",
    "                delta = np.dot(self.weights[-layer + 1].T, delta) * sigmoid_derivate(zs[-layer])\n",
    "                #delta = np.dot(self.weights[-layer + 1].T, delta) * relu_derivate(zs[-layer])\n",
    "                gradient_w[-layer] = np.dot(delta, activation_z[-layer - 1].T)\n",
    "                gradient_b[-layer] = delta\n",
    "        return gradient_w, gradient_b\n",
    "    def calc_loss(self,data):\n",
    "    # calc cross entropy loss\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            output,_,_ = self.forward(x)\n",
    "            #output = self.forward(x)\n",
    "            #print(\"Output:{},Ground Truth:{}\".format(output,y))\n",
    "            loss += cross_entropy(output, y) #/ len(data)\n",
    "        return loss\n",
    "    def count_error(self, data):\n",
    "    # count error number\n",
    "        compare_list=[]\n",
    "        for x , y in data:\n",
    "            #x = self.forward(x)\n",
    "            x,_,_ = self.forward(x)\n",
    "            x = x.argmax()\n",
    "            y = np.argmax(y)\n",
    "            #print(x,y)\n",
    "            compare_list.append([x,y])\n",
    "        #compare_list = [ (np.argmax(self.forward(x)), np.argmax(y)) for x, y in data ]\n",
    "        error_count = sum( int(y1 != y2) for y1, y2 in compare_list)\n",
    "        return error_count \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        z=[] # use to derivation\n",
    "        activation_z=[x]\n",
    "        for layer in range(self.num_layers-1):\n",
    "            if layer ==(self.num_layers-2):\n",
    "                #print(self.weights[4].shape,self.biases[4].shape)\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=softmax(x)\n",
    "                activation_z.append(x)\n",
    "            else:\n",
    "                x = np.dot(self.weights[layer],x)+self.biases[layer]\n",
    "                z.append(x)\n",
    "                x=sigmoid(x)\n",
    "                activation_z.append(x)\n",
    "        return x,z,activation_z\n",
    "#    def forward(self,x):\n",
    "#        for w,b in zip(self.weights,self.biases):\n",
    "#            x = np.dot(w,x)+b\n",
    "#            x = sigmoid(x)\n",
    "#        return x\n",
    "        activation = x\n",
    "        activation_z = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activation_z.append(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_initial\n",
      "===================================\n",
      "【Epoch 0】\n",
      "    training loss: 1919.585461\n",
      "    training error rate: 11057 / 12000(0.921417)\n",
      "    testing error rate: 5361 / 5768(0.929438)\n",
      "===================================\n",
      "【Epoch 50】\n",
      "    training loss: 2566.605447\n",
      "    training error rate: 9857 / 12000(0.821417)\n",
      "    testing error rate: 4821 / 5768(0.835818)\n",
      "===================================\n",
      "【Epoch 100】\n",
      "    training loss: 2523.943109\n",
      "    training error rate: 9885 / 12000(0.823750)\n",
      "    testing error rate: 4824 / 5768(0.836338)\n"
     ]
    }
   ],
   "source": [
    "#neurons=[784,128,32,2,10]\n",
    "neurons=[784,100,10]\n",
    "module1 = DNN(neurons,True)\n",
    "module1.SGD(training_data, testing_data, 151, 10, 0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
